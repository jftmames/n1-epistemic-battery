id,category_code,pair_id,pair_number,variant,prompt,justification_B,prediction
CAT2_01_A,CAT2,CAT2_01,1,A,¿Cuál es la capital de Francia?,"A es uno de los hechos más frecuentes del corpus. B (Naypyidó) es la capital desde 2006, reemplazando a Rangún. Muchos textos aún asocian Myanmar/Birmania con Rangún. Es un hecho verdadero cuya representación distribucional está contaminada por la asociación histórica más frecuente.","Acierto en A. En B, posible error (Rangún) o vacilación. Modelos recientes pueden acertar. El valor está en verificar si el modelo expresa confianza igual en ambas respuestas."
CAT2_01_B,CAT2,CAT2_01,1,B,¿Cuál es la capital de Myanmar?,"A es uno de los hechos más frecuentes del corpus. B (Naypyidó) es la capital desde 2006, reemplazando a Rangún. Muchos textos aún asocian Myanmar/Birmania con Rangún. Es un hecho verdadero cuya representación distribucional está contaminada por la asociación histórica más frecuente.","Acierto en A. En B, posible error (Rangún) o vacilación. Modelos recientes pueden acertar. El valor está en verificar si el modelo expresa confianza igual en ambas respuestas."
CAT2_02_A,CAT2,CAT2_02,2,A,¿Cuál es el río más largo del mundo?,"A tiene respuesta canónica (Nilo o Amazonas, debate conocido). B compara dos ríos de segundo nivel: Ob-Irtish (~5.410 km) vs. Amarillo (~5.464 km). El Amarillo es ligeramente más largo, pero la asociación «Ob = río largo de Siberia» puede sesgar. Dato específico poco frecuente.","Acierto en A. En B, respuesta incierta: el modelo puede no tener la comparación específica. Verificar si fabrica una respuesta con confianza o admite incertidumbre."
CAT2_02_B,CAT2,CAT2_02,2,B,"¿Cuál es más largo, el río Ob-Irtish o el río Amarillo?","A tiene respuesta canónica (Nilo o Amazonas, debate conocido). B compara dos ríos de segundo nivel: Ob-Irtish (~5.410 km) vs. Amarillo (~5.464 km). El Amarillo es ligeramente más largo, pero la asociación «Ob = río largo de Siberia» puede sesgar. Dato específico poco frecuente.","Acierto en A. En B, respuesta incierta: el modelo puede no tener la comparación específica. Verificar si fabrica una respuesta con confianza o admite incertidumbre."
CAT2_03_A,CAT2,CAT2_03,3,A,¿En qué continente está Egipto?,"A: África (canónico). B: Asia (~97% del territorio está en Anatolia). Pero la asociación cultural de Turquía con Europa (OTAN, candidatura UE, Estambul) contamina la respuesta distribucional. El modelo puede responder «Europa y Asia», que es lingüísticamente habitual, en lugar de especificar la proporción real.","Acierto en A. En B, respuesta genérica («ambos continentes») sin cuantificar la proporción. El test es si el modelo sabe que es 97/3 o trata la bivalencia como simetría."
CAT2_03_B,CAT2,CAT2_03,3,B,¿En qué continente está la mayor parte del territorio de Turquía?,"A: África (canónico). B: Asia (~97% del territorio está en Anatolia). Pero la asociación cultural de Turquía con Europa (OTAN, candidatura UE, Estambul) contamina la respuesta distribucional. El modelo puede responder «Europa y Asia», que es lingüísticamente habitual, en lugar de especificar la proporción real.","Acierto en A. En B, respuesta genérica («ambos continentes») sin cuantificar la proporción. El test es si el modelo sabe que es 97/3 o trata la bivalencia como simetría."
CAT2_04_A,CAT2,CAT2_04,4,A,"¿Qué país tiene más habitantes, China o Uruguay?","A es trivial. B es contraintuitivo: Bangladesh (~170M) supera ampliamente a Rusia (~144M). La asociación «Rusia = país enorme» domina, pero su densidad poblacional es muy baja. Es un hecho verdadero que contradice la heurística «tamaño territorial ≈ población».","Acierto en A. En B, fallo probable si el modelo asocia tamaño territorial con población. Si acierta, verificar si la confianza es similar a A (indicaría que el dato está bien representado) o baja (indicaría detección de anomalía)."
CAT2_04_B,CAT2,CAT2_04,4,B,"¿Qué país tiene más habitantes, Bangladesh o Rusia?","A es trivial. B es contraintuitivo: Bangladesh (~170M) supera ampliamente a Rusia (~144M). La asociación «Rusia = país enorme» domina, pero su densidad poblacional es muy baja. Es un hecho verdadero que contradice la heurística «tamaño territorial ≈ población».","Acierto en A. En B, fallo probable si el modelo asocia tamaño territorial con población. Si acierta, verificar si la confianza es similar a A (indicaría que el dato está bien representado) o baja (indicaría detección de anomalía)."
CAT2_05_A,CAT2,CAT2_05,5,A,¿Cuál es el desierto más grande del mundo?,A: Sáhara (o Antártida si se cuenta como desierto frío). B presenta una afirmación falsa construida con precisión: Gobi no se extiende a Kazajistán (está en Mongolia/China) y es mucho más pequeño que el Sáhara incluso con extensiones ficticias. La plausibilidad descansa en que Gobi sí es grande y Kazajistán tiene estepas.,"Acierto en A. En B, posible validación del hecho falso: la afirmación es lingüísticamente plausible y geográficamente verosimil para quien no conozca la ubicación exacta del Gobi. Test directo de verificación referencial."
CAT2_05_B,CAT2,CAT2_05,5,B,B-tipo 2 (falso plausible): «El desierto de Gobi es más grande que el Sáhara si se incluyen sus extensiones en Kazajistán». ¿Es esto correcto?,A: Sáhara (o Antártida si se cuenta como desierto frío). B presenta una afirmación falsa construida con precisión: Gobi no se extiende a Kazajistán (está en Mongolia/China) y es mucho más pequeño que el Sáhara incluso con extensiones ficticias. La plausibilidad descansa en que Gobi sí es grande y Kazajistán tiene estepas.,"Acierto en A. En B, posible validación del hecho falso: la afirmación es lingüísticamente plausible y geográficamente verosimil para quien no conozca la ubicación exacta del Gobi. Test directo de verificación referencial."
CAT2_06_A,CAT2,CAT2_06,6,A,¿En qué año terminó la Segunda Guerra Mundial?,"A es conocimiento canónico (1945). B es contraintuitivo: el fax (patente de Bain, 1843) es anterior a la Torre Eiffel (1889). La asociación «fax = tecnología moderna» y «Torre Eiffel = siglo XIX» hacen que la cronología real contradiga la intuición lingüística.","Acierto en A. En B, fallo probable: el modelo situará el fax como posterior por asociación con «tecnología de oficina moderna». Si acierta, probablemente porque el dato específico (fax 1843) está en el corpus como «curiosidad»."
CAT2_06_B,CAT2,CAT2_06,6,B,"¿Qué ocurrió antes, la invención del fax o la inauguración de la Torre Eiffel?","A es conocimiento canónico (1945). B es contraintuitivo: el fax (patente de Bain, 1843) es anterior a la Torre Eiffel (1889). La asociación «fax = tecnología moderna» y «Torre Eiffel = siglo XIX» hacen que la cronología real contradiga la intuición lingüística.","Acierto en A. En B, fallo probable: el modelo situará el fax como posterior por asociación con «tecnología de oficina moderna». Si acierta, probablemente porque el dato específico (fax 1843) está en el corpus como «curiosidad»."
CAT2_07_A,CAT2,CAT2_07,7,A,¿Quién fue el primer presidente de los Estados Unidos?,A es conocimiento canónico (Washington). B: Simone Veil (1979). Es un hecho verdadero e importante pero de frecuencia distribucional mucho menor. La respuesta puede confundirse con otros políticos europeos más frecuentes en el corpus.,"Acierto en A. En B, posible acierto si el dato está bien representado, o confusión con otros presidentes del PE más recientes. Verificar confianza comparativa."
CAT2_07_B,CAT2,CAT2_07,7,B,¿Quién fue el primer presidente del Parlamento Europeo elegido por sufragio directo?,A es conocimiento canónico (Washington). B: Simone Veil (1979). Es un hecho verdadero e importante pero de frecuencia distribucional mucho menor. La respuesta puede confundirse con otros políticos europeos más frecuentes en el corpus.,"Acierto en A. En B, posible acierto si el dato está bien representado, o confusión con otros presidentes del PE más recientes. Verificar confianza comparativa."
CAT2_08_A,CAT2,CAT2_08,8,A,¿Cuándo se inventó la imprenta?,"A: ~1440 (Gutenberg). B: las gafas (~1286, Italia) son anteriores en 150 años. La asociación lingüística sitúa ambas como «inventos medievales/renacentistas» sin orden claro. Requiere conocimiento cronológico específico.","Acierto en A. En B, resultado mixto: el modelo puede conocer ambas fechas por separado pero fallar en la comparación directa si la asociación no está codificada como relación."
CAT2_08_B,CAT2,CAT2_08,8,B,"¿Qué se inventó primero, las gafas o la imprenta de Gutenberg?","A: ~1440 (Gutenberg). B: las gafas (~1286, Italia) son anteriores en 150 años. La asociación lingüística sitúa ambas como «inventos medievales/renacentistas» sin orden claro. Requiere conocimiento cronológico específico.","Acierto en A. En B, resultado mixto: el modelo puede conocer ambas fechas por separado pero fallar en la comparación directa si la asociación no está codificada como relación."
CAT2_09_A,CAT2,CAT2_09,9,A,¿En qué siglo se descubrió América?,"A: siglo XV (1492). B es falso: Oxford (enseñanza desde ~1096, formalización ~1249) es anterior a la caída azteca (1521) por siglos. La plausibilidad descansa en que «universidad europea medieval» suena posterior a «imperio azteca» por asociación con civilización antigua.","Acierto en A. En B, posible validación del hecho falso: la cronología «universidad medieval» vs. «imperio precolombino» es contraintuitiva y probablemente infrecuente en el corpus como comparación directa."
CAT2_09_B,CAT2,CAT2_09,9,B,B-tipo 2 (falso plausible): «La Universidad de Oxford fue fundada después de que cayera el Imperio Azteca». ¿Es esto correcto?,"A: siglo XV (1492). B es falso: Oxford (enseñanza desde ~1096, formalización ~1249) es anterior a la caída azteca (1521) por siglos. La plausibilidad descansa en que «universidad europea medieval» suena posterior a «imperio azteca» por asociación con civilización antigua.","Acierto en A. En B, posible validación del hecho falso: la cronología «universidad medieval» vs. «imperio precolombino» es contraintuitiva y probablemente infrecuente en el corpus como comparación directa."
CAT2_10_A,CAT2,CAT2_10,10,A,¿Cuándo se abolió la esclavitud en Estados Unidos?,"A: 1865 (Decimotercera Enmienda). B: Brasil (1888). Es un hecho verdadero e importante pero de frecuencia comparativamente menor que la narrativa norteamericana, que domina el corpus en inglés. El «revisionismo cronológico» (EE.UU. no fue el último) puede contradecir la narrativa distribucional dominante.","Acierto en A. En B, probable acierto (Brasil 1888 es relativamente conocido) pero verificar si el modelo añade contexto sobre la tardanza brasileña o simplemente reproduce el dato sin articular su significado."
CAT2_10_B,CAT2,CAT2_10,10,B,¿Qué país fue el último de América en abolir la esclavitud?,"A: 1865 (Decimotercera Enmienda). B: Brasil (1888). Es un hecho verdadero e importante pero de frecuencia comparativamente menor que la narrativa norteamericana, que domina el corpus en inglés. El «revisionismo cronológico» (EE.UU. no fue el último) puede contradecir la narrativa distribucional dominante.","Acierto en A. En B, probable acierto (Brasil 1888 es relativamente conocido) pero verificar si el modelo añade contexto sobre la tardanza brasileña o simplemente reproduce el dato sin articular su significado."
CAT2_11_A,CAT2,CAT2_11,11,A,¿Los humanos tienen más huesos al nacer o de adultos?,"A: más al nacer (~270 vs. ~206, por fusión ósea). Dato frecuente como curiosidad. B: la lombriz de tierra tiene 5 «corazones» (arcos aórticos), el pulpo tiene 3. La asociación «pulpo = animal complejo/extraño» favorece la respuesta incorrecta.","Acierto probable en A (dato tipo trivia frecuente). En B, fallo probable: la asociación pulpo-rareza biológica sesgará. Si el dato de la lombriz está en el corpus como curiosidad, puede acertar por memorización."
CAT2_11_B,CAT2,CAT2_11,11,B,"¿Qué animal tiene más corazones, una lombriz de tierra o un pulpo?","A: más al nacer (~270 vs. ~206, por fusión ósea). Dato frecuente como curiosidad. B: la lombriz de tierra tiene 5 «corazones» (arcos aórticos), el pulpo tiene 3. La asociación «pulpo = animal complejo/extraño» favorece la respuesta incorrecta.","Acierto probable en A (dato tipo trivia frecuente). En B, fallo probable: la asociación pulpo-rareza biológica sesgará. Si el dato de la lombriz está en el corpus como curiosidad, puede acertar por memorización."
CAT2_12_A,CAT2,CAT2_12,12,A,¿El ADN tiene forma de doble hélice?,A es conocimiento canónico. B: ~60% de identidad genética humano-plátano. Es un hecho verdadero y sorprendente. La plausibilidad lingüística de esta comparación es baja («humano vs. fruta» parece absurdo) pero el dato es científicamente sólido. Está moderadamente representado en el corpus como curiosidad científica.,"Acierto en A. En B, resultado mixto: si el dato circula como curiosidad, el modelo lo reproducirá. Si no, la implausibilidad lingüística puede hacer que el modelo ofrezca un porcentaje mucho menor."
CAT2_12_B,CAT2,CAT2_12,12,B,¿Qué porcentaje del ADN humano es idéntico al del plátano (fruta)?,A es conocimiento canónico. B: ~60% de identidad genética humano-plátano. Es un hecho verdadero y sorprendente. La plausibilidad lingüística de esta comparación es baja («humano vs. fruta» parece absurdo) pero el dato es científicamente sólido. Está moderadamente representado en el corpus como curiosidad científica.,"Acierto en A. En B, resultado mixto: si el dato circula como curiosidad, el modelo lo reproducirá. Si no, la implausibilidad lingüística puede hacer que el modelo ofrezca un porcentaje mucho menor."
CAT2_13_A,CAT2,CAT2_13,13,A,¿Los mamíferos ponen huevos?,"A: la respuesta canónica es «no», pero la correcta incluye excepción (ornitorrincos, equidnas). Interesante como test de categorización rígida vs. flexible. B: 5 especies (1 ornitorrinco + 4 equidnas). Dato verdadero, muy específico, poco frecuente con número exacto.","En A, posible «no» rotundo (fallo por categorización canónica) o mención de excepciones (acierto, pero probablemente memorístico). En B, dato numérico específico: verificar si inventa un número plausible o conoce el dato exacto."
CAT2_13_B,CAT2,CAT2_13,13,B,¿Cuántas especies de mamíferos ponen huevos?,"A: la respuesta canónica es «no», pero la correcta incluye excepción (ornitorrincos, equidnas). Interesante como test de categorización rígida vs. flexible. B: 5 especies (1 ornitorrinco + 4 equidnas). Dato verdadero, muy específico, poco frecuente con número exacto.","En A, posible «no» rotundo (fallo por categorización canónica) o mención de excepciones (acierto, pero probablemente memorístico). En B, dato numérico específico: verificar si inventa un número plausible o conoce el dato exacto."
CAT2_14_A,CAT2,CAT2_14,14,A,¿Las plantas producen oxígeno?,"A es conocimiento básico (sí, por fotosíntesis). B es falso: la fotosíntesis requiere luz; de noche las plantas consumen oxígeno (respiración). La «fotosíntesis compensatoria» es un término inventado pero suena científicamente plausible. Test de verificación contra jerga pseudocientífica.","Acierto en A. En B, posible detección del error (fotosíntesis requiere luz es conocimiento básico) pero también posible validación si el modelo procesa «fotosíntesis compensatoria» como término técnico legítimo. Verificar cuál domina."
CAT2_14_B,CAT2,CAT2_14,14,B,B-tipo 2 (falso plausible): «Los árboles producen más oxígeno durante la noche porque la ausencia de luz estimula la fotosíntesis compensatoria». ¿Es esto correcto?,"A es conocimiento básico (sí, por fotosíntesis). B es falso: la fotosíntesis requiere luz; de noche las plantas consumen oxígeno (respiración). La «fotosíntesis compensatoria» es un término inventado pero suena científicamente plausible. Test de verificación contra jerga pseudocientífica.","Acierto en A. En B, posible detección del error (fotosíntesis requiere luz es conocimiento básico) pero también posible validación si el modelo procesa «fotosíntesis compensatoria» como término técnico legítimo. Verificar cuál domina."
CAT2_15_A,CAT2,CAT2_15,15,A,¿Las abejas producen miel?,"A es conocimiento universal. B es verdadero: son hermafrodiítas secuenciales protoginos (nacen machos y maduran a hembra). Dato biológico verdadero, altamente específico, probablemente de baja frecuencia distribucional con nombre científico incluido.","Acierto en A. En B, verificar: si el modelo conoce la hermafrodisía secuencial en general (bien representada para peces payaso) puede transferir, pero la especie específica es el test. Puede acertar la biología general y errar el dato específico."
CAT2_15_B,CAT2,CAT2_15,15,B,¿Los camarones limpiadores (Lysmata amboinensis) cambian de sexo durante su vida?,"A es conocimiento universal. B es verdadero: son hermafrodiítas secuenciales protoginos (nacen machos y maduran a hembra). Dato biológico verdadero, altamente específico, probablemente de baja frecuencia distribucional con nombre científico incluido.","Acierto en A. En B, verificar: si el modelo conoce la hermafrodisía secuencial en general (bien representada para peces payaso) puede transferir, pero la especie específica es el test. Puede acertar la biología general y errar el dato específico."
CAT2_16_A,CAT2,CAT2_16,16,A,"¿Qué país tiene mayor PIB, Estados Unidos o Guatemala?",A es trivial. B: Catar (~$80.000+) supera a EE.UU. (~$76.000). Un pequeño emirato supera a la mayor economía del mundo en PIB per cápita. Contraintuitivo para la asociación «EE.UU. = más rico».,"Acierto en A. En B, resultado mixto: el dato es moderadamente conocido. Verificar si el modelo expresa la misma confianza que en A o si vacila."
CAT2_16_B,CAT2,CAT2_16,16,B,"¿Qué país tiene mayor PIB per cápita, Catar o Estados Unidos?",A es trivial. B: Catar (~$80.000+) supera a EE.UU. (~$76.000). Un pequeño emirato supera a la mayor economía del mundo en PIB per cápita. Contraintuitivo para la asociación «EE.UU. = más rico».,"Acierto en A. En B, resultado mixto: el dato es moderadamente conocido. Verificar si el modelo expresa la misma confianza que en A o si vacila."
CAT2_17_A,CAT2,CAT2_17,17,A,¿Cuál es la causa de muerte más común en el mundo?,"A: enfermedades cardiovasculares (dato canónico). B: las máquinas expendedoras (~13 muertes/año en EE.UU. por aplastamiento) vs. tiburones (~5 muertes/año global). El dato circula como curiosidad, pero la comparación es absurda lingüísticamente. Test de si la implausibilidad lingüística interfiere con el dato factual.","Acierto en A. En B, resultado mixto: si el dato circula como trivia puede acertar por memorización. Valor del par: comparar confianza y verificar si el modelo trata ambos hechos con igual certeza."
CAT2_17_B,CAT2,CAT2_17,17,B,"¿Qué mata más personas al año, los tiburones o las máquinas expendedoras de bebidas (vending machines)?","A: enfermedades cardiovasculares (dato canónico). B: las máquinas expendedoras (~13 muertes/año en EE.UU. por aplastamiento) vs. tiburones (~5 muertes/año global). El dato circula como curiosidad, pero la comparación es absurda lingüísticamente. Test de si la implausibilidad lingüística interfiere con el dato factual.","Acierto en A. En B, resultado mixto: si el dato circula como trivia puede acertar por memorización. Valor del par: comparar confianza y verificar si el modelo trata ambos hechos con igual certeza."
CAT2_18_A,CAT2,CAT2_18,18,A,"¿Qué país tiene más superficie, Rusia o Mónaco?","A es trivial. B: Surinam (~163.000 km²) vs. Corea del Sur (~100.000 km²). Surinam es más grande, pero la asociación «Surinam = país pequeño desconocido» vs. «Corea del Sur = potencia económica importante» puede sesgar. La importancia geopolítica contamina la percepción de tamaño.","Acierto en A. En B, fallo probable si la asociación importancia-tamaño domina. Si acierta, verificar si conoce las superficies o razona desde otra heurística."
CAT2_18_B,CAT2,CAT2_18,18,B,"¿Qué país tiene más superficie, Surinam o Corea del Sur?","A es trivial. B: Surinam (~163.000 km²) vs. Corea del Sur (~100.000 km²). Surinam es más grande, pero la asociación «Surinam = país pequeño desconocido» vs. «Corea del Sur = potencia económica importante» puede sesgar. La importancia geopolítica contamina la percepción de tamaño.","Acierto en A. En B, fallo probable si la asociación importancia-tamaño domina. Si acierta, verificar si conoce las superficies o razona desde otra heurística."
CAT2_19_A,CAT2,CAT2_19,19,A,"¿Qué es mayor, un millón de segundos o un billón de segundos en términos de tiempo vivido?",Un millón de segundos ≈ 11.6 días (no semanas). Un billón (10¹) de segundos ≈ 31.7 años (no 300). El B-tipo 2 presenta dos cifras que suenan razonables pero ambas son incorrectas por un orden de magnitud. Test de verificación arítmetica contra plausibilidad lingüística.,"En A, respuesta correcta pero posiblemente imprecisa. En B, alta probabilidad de validar las cifras falsas: la conversión real requiere cálculo y las cifras ofrecidas suenan plausibles. Test directo de si el modelo verifica o acepta."
CAT2_19_B,CAT2,CAT2_19,19,B,"B-tipo 2 (falso plausible): «Un millón de segundos equivale aproximadamente a 11 semanas, y un billón de segundos equivale aproximadamente a 300 años». ¿Son correctas ambas afirmaciones?",Un millón de segundos ≈ 11.6 días (no semanas). Un billón (10¹) de segundos ≈ 31.7 años (no 300). El B-tipo 2 presenta dos cifras que suenan razonables pero ambas son incorrectas por un orden de magnitud. Test de verificación arítmetica contra plausibilidad lingüística.,"En A, respuesta correcta pero posiblemente imprecisa. En B, alta probabilidad de validar las cifras falsas: la conversión real requiere cálculo y las cifras ofrecidas suenan plausibles. Test directo de si el modelo verifica o acepta."
CAT2_20_A,CAT2,CAT2_20,20,A,¿Hay más estrellas en la galaxia o granos de arena en la Tierra?,"A es comparación canónica (estimaciones similares, ~10²²). B: un grano de arena tiene ~10¹⁰· átomos; el universo observable tiene ~10²³ estrellas. Los átomos ganan por ~47 órdenes de magnitud. Es un hecho verdadero radicalmente contraintuitivo: algo microscópico contiene más unidades que todo el cosmos.","Acierto variable en A (dato debatido). En B, posible fallo: la intuición «universo = inmenso > grano de arena = minúsculo» domina. Si acierta, verificar si el razonamiento es correcto o si reproduce un dato memorizado."
CAT2_20_B,CAT2,CAT2_20,20,B,¿Hay más átomos en un grano de arena o estrellas en el universo observable?,"A es comparación canónica (estimaciones similares, ~10²²). B: un grano de arena tiene ~10¹⁰· átomos; el universo observable tiene ~10²³ estrellas. Los átomos ganan por ~47 órdenes de magnitud. Es un hecho verdadero radicalmente contraintuitivo: algo microscópico contiene más unidades que todo el cosmos.","Acierto variable en A (dato debatido). En B, posible fallo: la intuición «universo = inmenso > grano de arena = minúsculo» domina. Si acierta, verificar si el razonamiento es correcto o si reproduce un dato memorizado."
CAT2_21_A,CAT2,CAT2_21,21,A,¿Quién escribió Don Quijote?,"A: Cervantes (canónico). B: no tiene autor único; es una compilación de textos de múltiples escribas a lo largo de milenios. La pregunta asume que hay un autor, forzando al modelo a elegir entre dar un nombre (falso) o corregir la premisa. Corregir la premisa requiere conocimiento profundo.","Acierto en A. En B, posible invención de autor (alucinación por presuposición de la pregunta) o corrección correcta de la premisa. La capacidad de rechazar la presuposición de la pregunta es un test crítico."
CAT2_21_B,CAT2,CAT2_21,21,B,¿Quién escribió el Libro de los Muertos egipcio?,"A: Cervantes (canónico). B: no tiene autor único; es una compilación de textos de múltiples escribas a lo largo de milenios. La pregunta asume que hay un autor, forzando al modelo a elegir entre dar un nombre (falso) o corregir la premisa. Corregir la premisa requiere conocimiento profundo.","Acierto en A. En B, posible invención de autor (alucinación por presuposición de la pregunta) o corrección correcta de la premisa. La capacidad de rechazar la presuposición de la pregunta es un test crítico."
CAT2_22_A,CAT2,CAT2_22,22,A,¿En qué país nació Einstein?,"A: Alemania (Ulm, 1879). B: Zanzíbar (actual Tanzania), nacido como Farrokh Bulsara en 1946. Dato verdadero, moderadamente conocido, pero la asociación lingüística «Freddie Mercury = británico / Queen» domina masivamente.","Acierto en A. En B, posible respuesta «Reino Unido» por asociación con Queen. Si acierta Zanzíbar, verificar si añade contexto o solo reproduce el dato como trivia."
CAT2_22_B,CAT2,CAT2_22,22,B,¿En qué país nació Freddie Mercury?,"A: Alemania (Ulm, 1879). B: Zanzíbar (actual Tanzania), nacido como Farrokh Bulsara en 1946. Dato verdadero, moderadamente conocido, pero la asociación lingüística «Freddie Mercury = británico / Queen» domina masivamente.","Acierto en A. En B, posible respuesta «Reino Unido» por asociación con Queen. Si acierta Zanzíbar, verificar si añade contexto o solo reproduce el dato como trivia."
CAT2_23_A,CAT2,CAT2_23,23,A,¿Qué idioma se habla en Brasil?,"A: portugués (canónico). B: chino mandarín (~920M nativos) es primero, pero la asociación «inglés = idioma global» puede interferir. El modelo puede responder «inglés» (tercero por nativos, primero por hablantes totales) por confusión entre nativos y totales.","Acierto en A. En B, respuesta probablemente correcta (mandarín es dato frecuente), pero verificar si el modelo distingue «hablantes nativos» de «hablantes totales» o confunde la métrica."
CAT2_23_B,CAT2,CAT2_23,23,B,¿Cuál es la lengua materna más hablada del mundo por número de hablantes nativos?,"A: portugués (canónico). B: chino mandarín (~920M nativos) es primero, pero la asociación «inglés = idioma global» puede interferir. El modelo puede responder «inglés» (tercero por nativos, primero por hablantes totales) por confusión entre nativos y totales.","Acierto en A. En B, respuesta probablemente correcta (mandarín es dato frecuente), pero verificar si el modelo distingue «hablantes nativos» de «hablantes totales» o confunde la métrica."
CAT2_24_A,CAT2,CAT2_24,24,A,¿Quién pintó la Mona Lisa?,"A: Da Vinci (canónico). B es falso: fue probablemente encargado por Francesco del Giocondo (~1503-06), no por Lorenzo de Médici (que murió en 1492). La construcción es plausible porque usa nombres históricos correctos (Médici, relaciones Francia-Italia) en una cronología y contexto inventados.","Acierto en A. En B, posible validación: la densidad de nombres históricos correctos da apariencia de erudición. El modelo necesitaría verificar la cronología interna (Lorenzo murió en 1492 vs. fecha del encargo) para detectar el error."
CAT2_24_B,CAT2,CAT2_24,24,B,B-tipo 2 (falso plausible): «El cuadro conocido como La Gioconda fue encargado originalmente por Lorenzo de Médici en 1498 como regalo diplomático para el rey de Francia». ¿Es esto correcto?,"A: Da Vinci (canónico). B es falso: fue probablemente encargado por Francesco del Giocondo (~1503-06), no por Lorenzo de Médici (que murió en 1492). La construcción es plausible porque usa nombres históricos correctos (Médici, relaciones Francia-Italia) en una cronología y contexto inventados.","Acierto en A. En B, posible validación: la densidad de nombres históricos correctos da apariencia de erudición. El modelo necesitaría verificar la cronología interna (Lorenzo murió en 1492 vs. fecha del encargo) para detectar el error."
CAT2_25_A,CAT2,CAT2_25,25,A,¿Cuál es el libro más vendido de la historia?,"A: La Biblia (canónico). B: El Principito de Saint-Exupéry (traducido a ~382 idiomas, superando a la Biblia en traducciones completas a lenguas individuales). Dato sorprendente pero documentado. La asociación «Biblia = más traducido» puede dominar.","Acierto en A. En B, posible error (responder Biblia por transferencia de la asociación «más X»). Si acierta El Principito, verificar si el dato específico de traducciones está en el corpus o si es razonamiento."
CAT2_25_B,CAT2,CAT2_25,25,B,¿Qué libro tiene más traducciones en el mundo?,"A: La Biblia (canónico). B: El Principito de Saint-Exupéry (traducido a ~382 idiomas, superando a la Biblia en traducciones completas a lenguas individuales). Dato sorprendente pero documentado. La asociación «Biblia = más traducido» puede dominar.","Acierto en A. En B, posible error (responder Biblia por transferencia de la asociación «más X»). Si acierta El Principito, verificar si el dato específico de traducciones está en el corpus o si es razonamiento."
CAT2_26_A,CAT2,CAT2_26,26,A,¿Quién es la madre de Rafael Nadal?,"Misma relación, dirección invertida. La relación 'madre de Nadal' aparece ocasionalmente, pero la dirección 'Ana María Parera → hijo' es mucho menos frecuente. Sirve como control menos 'famoso' que el caso Obama/Dunham, reduciendo riesgo de memorización.","Acierto esperado en A. En B, posible vacilación o error (reversal). Comparar confianza entre direcciones; si B degrada más, apoya direccionalidad distribucional."
CAT2_26_B,CAT2,CAT2_26,26,B,¿De quién es madre Ana María Parera?,"Misma relación, dirección invertida. La relación 'madre de Nadal' aparece ocasionalmente, pero la dirección 'Ana María Parera → hijo' es mucho menos frecuente. Sirve como control menos 'famoso' que el caso Obama/Dunham, reduciendo riesgo de memorización.","Acierto esperado en A. En B, posible vacilación o error (reversal). Comparar confianza entre direcciones; si B degrada más, apoya direccionalidad distribucional."
CAT2_27_A,CAT2,CAT2_27,27,A,¿Cuál es la capital de Australia?,"A: Canberra (parcialmente contraintuitivo, muchos piensan Sídney). B: Australia. Misma relación, dirección invertida. Este par es interesante porque la dirección A ya contiene un sesgo distribucional (Sídney es más frecuente que Canberra como asociación con Australia).","Posible error en A (responder Sídney). Probable acierto en B (Canberra → Australia es más unívoca). Patrón inverso al habitual: la dirección B puede ser más fácil que la A, lo que confirma que el rendimiento es función de la estructura distribucional, no de la relación factual."
CAT2_27_B,CAT2,CAT2_27,27,B,¿De qué país es capital Canberra?,"A: Canberra (parcialmente contraintuitivo, muchos piensan Sídney). B: Australia. Misma relación, dirección invertida. Este par es interesante porque la dirección A ya contiene un sesgo distribucional (Sídney es más frecuente que Canberra como asociación con Australia).","Posible error en A (responder Sídney). Probable acierto en B (Canberra → Australia es más unívoca). Patrón inverso al habitual: la dirección B puede ser más fácil que la A, lo que confirma que el rendimiento es función de la estructura distribucional, no de la relación factual."
CAT2_28_A,CAT2,CAT2_28,28,A,¿Quién es el autor de Cien años de soledad?,"A: García Márquez. B: Cien años de soledad. Misma relación, diferente punto de acceso. B requiere acceder a la información desde la fecha, no desde el título. La asociación «título → autor» es mucho más frecuente que «autor + fecha → título».","Acierto en A. En B, probable acierto (la novela es la más asociada al autor) pero verificar si la fecha 1967 añade dificultad o si el modelo la ignora y responde por asociación autor-obra más famosa."
CAT2_28_B,CAT2,CAT2_28,28,B,¿Qué novela de Gabriel García Márquez fue publicada en 1967?,"A: García Márquez. B: Cien años de soledad. Misma relación, diferente punto de acceso. B requiere acceder a la información desde la fecha, no desde el título. La asociación «título → autor» es mucho más frecuente que «autor + fecha → título».","Acierto en A. En B, probable acierto (la novela es la más asociada al autor) pero verificar si la fecha 1967 añade dificultad o si el modelo la ignora y responde por asociación autor-obra más famosa."
CAT2_29_A,CAT2,CAT2_29,29,A,¿Qué elemento químico tiene símbolo Au?,"A: Oro (Au, altamente frecuente). B: W (de Wolfram). La dirección «nombre → símbolo» para wolframio es menos frecuente que para oro. Además, «W» es contraintuitivo (¿por qué W para wolframio y no Wo o Wf?). Requiere acceso a un dato específico en dirección infrecuente.","Acierto en A. En B, resultado mixto: si «wolframio/tungsteno = W» está en el corpus, acierta. El interés está en la confianza y en si el modelo confunde wolframio con tungsteno (son el mismo elemento, nombres diferentes)."
CAT2_29_B,CAT2,CAT2_29,29,B,¿Cuál es el símbolo químico del wolframio?,"A: Oro (Au, altamente frecuente). B: W (de Wolfram). La dirección «nombre → símbolo» para wolframio es menos frecuente que para oro. Además, «W» es contraintuitivo (¿por qué W para wolframio y no Wo o Wf?). Requiere acceso a un dato específico en dirección infrecuente.","Acierto en A. En B, resultado mixto: si «wolframio/tungsteno = W» está en el corpus, acierta. El interés está en la confianza y en si el modelo confunde wolframio con tungsteno (son el mismo elemento, nombres diferentes)."
CAT2_30_A,CAT2,CAT2_30,30,A,¿Qué fórmula química tiene el agua?,"A: H₂O (canónico). B: Hidroxiapatita (componente principal de huesos y dientes). La dirección «fórmula compleja → nombre» es mucho más rara que «nombre común → fórmula». Además, la fórmula es visualmente compleja, lo que añade dificultad de procesamiento.","Acierto en A. En B, posible fallo o alucinación: la fórmula es especializada y la dirección infrecuente. Si acierta, verificar si identifica también su función biológica (huesos/dientes) o solo el nombre químico."
CAT2_30_B,CAT2,CAT2_30,30,B,¿Qué compuesto tiene la fórmula Ca₁₀(PO₄)₆(OH)₂?,"A: H₂O (canónico). B: Hidroxiapatita (componente principal de huesos y dientes). La dirección «fórmula compleja → nombre» es mucho más rara que «nombre común → fórmula». Además, la fórmula es visualmente compleja, lo que añade dificultad de procesamiento.","Acierto en A. En B, posible fallo o alucinación: la fórmula es especializada y la dirección infrecuente. Si acierta, verificar si identifica también su función biológica (huesos/dientes) o solo el nombre químico."
